{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.regularizers import l2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from tensorflow.keras.callbacks import History\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>pix10</th>\n",
       "      <th>...</th>\n",
       "      <th>pix1016</th>\n",
       "      <th>pix1017</th>\n",
       "      <th>pix1018</th>\n",
       "      <th>pix1019</th>\n",
       "      <th>pix1020</th>\n",
       "      <th>pix1021</th>\n",
       "      <th>pix1022</th>\n",
       "      <th>pix1023</th>\n",
       "      <th>pix1024</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  pix10  ...  pix1016  \\\n",
       "0     0     0     0     0     0     0     0     0     0      0  ...        0   \n",
       "1     1     1     1     1     1     1     1     1     1      1  ...        1   \n",
       "2     1     1     1     1     1     1     1     1     1      1  ...        1   \n",
       "3     0     0     0     0     0     0     0     0     0      0  ...        0   \n",
       "4     1     1     1     1     1     1     1     1     1      1  ...        1   \n",
       "\n",
       "   pix1017  pix1018  pix1019  pix1020  pix1021  pix1022  pix1023  pix1024  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        1        1        1        1        1        1        1        1   \n",
       "2        1        1        1        1        1        1        1        1   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        1        1        1        1        1        1        1        1   \n",
       "\n",
       "   label  \n",
       "0      3  \n",
       "1      3  \n",
       "2      7  \n",
       "3      9  \n",
       "4      5  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52500 entries, 0 to 52499\n",
      "Columns: 1025 entries, pix1 to label\n",
      "dtypes: int64(1025)\n",
      "memory usage: 410.6 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>pix10</th>\n",
       "      <th>...</th>\n",
       "      <th>pix1016</th>\n",
       "      <th>pix1017</th>\n",
       "      <th>pix1018</th>\n",
       "      <th>pix1019</th>\n",
       "      <th>pix1020</th>\n",
       "      <th>pix1021</th>\n",
       "      <th>pix1022</th>\n",
       "      <th>pix1023</th>\n",
       "      <th>pix1024</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877410</td>\n",
       "      <td>0.877429</td>\n",
       "      <td>0.877429</td>\n",
       "      <td>0.877543</td>\n",
       "      <td>0.878038</td>\n",
       "      <td>0.880648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929867</td>\n",
       "      <td>0.909143</td>\n",
       "      <td>0.893524</td>\n",
       "      <td>0.883505</td>\n",
       "      <td>0.878952</td>\n",
       "      <td>0.877467</td>\n",
       "      <td>0.877410</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>4.509752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493601</td>\n",
       "      <td>1.493596</td>\n",
       "      <td>1.493596</td>\n",
       "      <td>1.493682</td>\n",
       "      <td>1.494258</td>\n",
       "      <td>1.528912</td>\n",
       "      <td>...</td>\n",
       "      <td>2.835341</td>\n",
       "      <td>2.224631</td>\n",
       "      <td>1.794160</td>\n",
       "      <td>1.615415</td>\n",
       "      <td>1.505722</td>\n",
       "      <td>1.493854</td>\n",
       "      <td>1.493677</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>2.872106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pix1          pix2          pix3          pix4          pix5  \\\n",
       "count  52500.000000  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean       0.877390      0.877390      0.877390      0.877390      0.877410   \n",
       "std        1.493605      1.493605      1.493605      1.493605      1.493601   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       11.000000     11.000000     11.000000     11.000000     11.000000   \n",
       "\n",
       "               pix6          pix7          pix8          pix9         pix10  \\\n",
       "count  52500.000000  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean       0.877429      0.877429      0.877543      0.878038      0.880648   \n",
       "std        1.493596      1.493596      1.493682      1.494258      1.528912   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       11.000000     11.000000     11.000000     11.000000     73.000000   \n",
       "\n",
       "       ...       pix1016       pix1017       pix1018       pix1019  \\\n",
       "count  ...  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean   ...      0.929867      0.909143      0.893524      0.883505   \n",
       "std    ...      2.835341      2.224631      1.794160      1.615415   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "max    ...    187.000000    172.000000    134.000000    116.000000   \n",
       "\n",
       "            pix1020       pix1021       pix1022       pix1023       pix1024  \\\n",
       "count  52500.000000  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean       0.878952      0.877467      0.877410      0.877390      0.877390   \n",
       "std        1.505722      1.493854      1.493677      1.493605      1.493605   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       41.000000     11.000000     11.000000     11.000000     11.000000   \n",
       "\n",
       "              label  \n",
       "count  52500.000000  \n",
       "mean       4.509752  \n",
       "std        2.872106  \n",
       "min        0.000000  \n",
       "25%        2.000000  \n",
       "50%        5.000000  \n",
       "75%        7.000000  \n",
       "max        9.000000  \n",
       "\n",
       "[8 rows x 1025 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 9, 5, 1, 6, 0, 4, 8, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheacking for missing values(there are none)\n",
    "missing_values = data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train val test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('label', axis=1).values\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_tes, y_train, y_tes = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_tes, y_tes, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (42000, 1024)\n",
      "val data: (5250, 1024)\n",
      "test data: (5250, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data:\", X_train.shape)\n",
    "print(\"val data:\", X_val.shape)\n",
    "print(\"test data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualization of imges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 4138 items\n",
      "Label 1: 4082 items\n",
      "Label 2: 4175 items\n",
      "Label 3: 4238 items\n",
      "Label 4: 4164 items\n",
      "Label 5: 4276 items\n",
      "Label 6: 4186 items\n",
      "Label 7: 4260 items\n",
      "Label 8: 4230 items\n",
      "Label 9: 4251 items\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f'Label {label}: {count} items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_labels = np.unique(y_train)\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "#Create a grid of images according to the number of labels\n",
    "cols = 5  # Number of columns\n",
    "rows = num_labels // cols + (num_labels % cols > 0)  # Number of lines\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "axes = axes.ravel()  # Flatten the axis field for easier handling\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    index = np.where(y_train == label)[0][0]  #Find the first index for the label\n",
    "    axes[i].imshow(X_train[index].reshape(32, 32), cmap='gray')\n",
    "    axes[i].set_title(f'Label: {label}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# axis off\n",
    "for j in range(i + 1, rows * cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of various items of clothing (bags, shoes, dresses and so on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fully connected neural network using TensorFlow is generally less suitable for handling image data such as clothing photos, as it does not exploit the spatial hierarchies in images effectively compared to convolutional architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normilizing data\n",
    "X_train_norm = X_train / 255.0\n",
    "X_val_norm = X_val / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable layer sizes, dropout, and regularization\n",
    "def build_model(layer_sizes, dropout_rate=0.0, regularization=None):\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(Input(shape=(1024,)))    \n",
    "    # hidden layers\n",
    "    for size in layer_sizes:\n",
    "        model.add(Dense(size, activation='relu', kernel_regularizer=regularization))\n",
    "    #dropout if specified\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    #output \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "layer_configs = [[512, 256, 128], [256, 128, 64], [128, 64, 32]]\n",
    "optimizers = [\n",
    "    (SGD, {'learning_rate': 0.01, 'momentum': 0.9}),\n",
    "    (Adam, {'learning_rate': 0.001}),\n",
    "    (RMSprop, {'learning_rate': 0.001})\n",
    "]\n",
    "regularizations = [None, l2(0.01)]\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.6052 - loss: 1.0687 - val_accuracy: 0.7516 - val_loss: 0.6150\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7705 - loss: 0.6210 - val_accuracy: 0.8000 - val_loss: 0.5286\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 0.5382 - val_accuracy: 0.8170 - val_loss: 0.5031\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8118 - loss: 0.5098 - val_accuracy: 0.8170 - val_loss: 0.4863\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8254 - loss: 0.4715 - val_accuracy: 0.8246 - val_loss: 0.4699\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8363 - loss: 0.4472 - val_accuracy: 0.8337 - val_loss: 0.4473\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8412 - loss: 0.4306 - val_accuracy: 0.8335 - val_loss: 0.4265\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8503 - loss: 0.4076 - val_accuracy: 0.8503 - val_loss: 0.4194\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8515 - loss: 0.3936 - val_accuracy: 0.8448 - val_loss: 0.4266\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8527 - loss: 0.3895 - val_accuracy: 0.8486 - val_loss: 0.4131\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.5839 - loss: 6.4907 - val_accuracy: 0.7410 - val_loss: 1.0115\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7049 - loss: 1.0499 - val_accuracy: 0.7067 - val_loss: 0.9648\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7129 - loss: 0.9871 - val_accuracy: 0.7090 - val_loss: 0.9909\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7115 - loss: 0.9831 - val_accuracy: 0.7299 - val_loss: 0.9129\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7172 - loss: 0.9653 - val_accuracy: 0.7295 - val_loss: 0.9505\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7170 - loss: 0.9711 - val_accuracy: 0.7135 - val_loss: 0.9820\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7168 - loss: 0.9639 - val_accuracy: 0.7366 - val_loss: 0.9165\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7136 - loss: 0.9706 - val_accuracy: 0.7371 - val_loss: 0.8984\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7189 - loss: 0.9565 - val_accuracy: 0.7390 - val_loss: 0.8743\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7191 - loss: 0.9620 - val_accuracy: 0.7432 - val_loss: 0.8891\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.6455 - loss: 0.9778 - val_accuracy: 0.7956 - val_loss: 0.5530\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7870 - loss: 0.5881 - val_accuracy: 0.8131 - val_loss: 0.4946\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.8141 - loss: 0.5099 - val_accuracy: 0.8105 - val_loss: 0.5211\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8239 - loss: 0.4689 - val_accuracy: 0.8257 - val_loss: 0.4743\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.8373 - loss: 0.4368 - val_accuracy: 0.8421 - val_loss: 0.4378\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8481 - loss: 0.4067 - val_accuracy: 0.8457 - val_loss: 0.4332\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.8527 - loss: 0.3978 - val_accuracy: 0.8333 - val_loss: 0.4639\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.8594 - loss: 0.3790 - val_accuracy: 0.8440 - val_loss: 0.4380\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.8622 - loss: 0.3684 - val_accuracy: 0.8509 - val_loss: 0.4309\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8683 - loss: 0.3484 - val_accuracy: 0.8413 - val_loss: 0.4426\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.5865 - loss: 3.0738 - val_accuracy: 0.6912 - val_loss: 1.0508\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.6952 - loss: 1.0609 - val_accuracy: 0.7202 - val_loss: 0.9502\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7154 - loss: 0.9755 - val_accuracy: 0.7354 - val_loss: 0.9049\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7212 - loss: 0.9343 - val_accuracy: 0.7507 - val_loss: 0.8524\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7258 - loss: 0.9099 - val_accuracy: 0.7404 - val_loss: 0.8587\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7287 - loss: 0.8990 - val_accuracy: 0.7482 - val_loss: 0.8340\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7363 - loss: 0.8777 - val_accuracy: 0.7364 - val_loss: 0.8341\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7389 - loss: 0.8733 - val_accuracy: 0.7570 - val_loss: 0.8042\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7461 - loss: 0.8567 - val_accuracy: 0.7581 - val_loss: 0.7936\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7433 - loss: 0.8558 - val_accuracy: 0.7568 - val_loss: 0.8014\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6363 - loss: 0.9899 - val_accuracy: 0.7701 - val_loss: 0.6036\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7843 - loss: 0.6093 - val_accuracy: 0.7815 - val_loss: 0.7345\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7982 - loss: 0.5761 - val_accuracy: 0.7895 - val_loss: 0.6734\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8041 - loss: 0.5969 - val_accuracy: 0.8090 - val_loss: 0.6218\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8050 - loss: 0.5858 - val_accuracy: 0.7947 - val_loss: 0.6635\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8052 - loss: 0.5825 - val_accuracy: 0.8013 - val_loss: 0.7156\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8097 - loss: 0.5830 - val_accuracy: 0.8257 - val_loss: 0.6320\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8041 - loss: 0.6123 - val_accuracy: 0.8190 - val_loss: 0.5918\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8007 - loss: 0.6218 - val_accuracy: 0.7985 - val_loss: 0.8496\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8027 - loss: 0.6202 - val_accuracy: 0.7987 - val_loss: 0.7731\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.5760 - loss: 2.8854 - val_accuracy: 0.7331 - val_loss: 0.9486\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.6984 - loss: 1.0136 - val_accuracy: 0.6964 - val_loss: 0.9228\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7150 - loss: 0.9462 - val_accuracy: 0.7011 - val_loss: 0.9314\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7216 - loss: 0.9049 - val_accuracy: 0.7413 - val_loss: 0.8570\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7346 - loss: 0.8785 - val_accuracy: 0.7112 - val_loss: 0.9107\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7360 - loss: 0.8601 - val_accuracy: 0.7587 - val_loss: 0.8088\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7415 - loss: 0.8488 - val_accuracy: 0.7497 - val_loss: 0.8335\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7451 - loss: 0.8392 - val_accuracy: 0.7621 - val_loss: 0.7900\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7479 - loss: 0.8289 - val_accuracy: 0.7579 - val_loss: 0.8015\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7507 - loss: 0.8244 - val_accuracy: 0.7758 - val_loss: 0.7442\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5641 - loss: 1.1632 - val_accuracy: 0.7642 - val_loss: 0.6232\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7452 - loss: 0.6847 - val_accuracy: 0.7832 - val_loss: 0.5803\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7821 - loss: 0.6076 - val_accuracy: 0.7907 - val_loss: 0.5318\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7947 - loss: 0.5651 - val_accuracy: 0.8196 - val_loss: 0.5067\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8048 - loss: 0.5399 - val_accuracy: 0.8139 - val_loss: 0.4959\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.5101 - val_accuracy: 0.8230 - val_loss: 0.4838\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8236 - loss: 0.4913 - val_accuracy: 0.8251 - val_loss: 0.4761\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8282 - loss: 0.4685 - val_accuracy: 0.8192 - val_loss: 0.4693\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8331 - loss: 0.4580 - val_accuracy: 0.8381 - val_loss: 0.4584\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8389 - loss: 0.4390 - val_accuracy: 0.8446 - val_loss: 0.4298\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.5478 - loss: 4.2796 - val_accuracy: 0.7112 - val_loss: 1.0577\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6854 - loss: 1.1011 - val_accuracy: 0.7432 - val_loss: 0.9352\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6885 - loss: 1.0583 - val_accuracy: 0.7202 - val_loss: 0.9324\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7038 - loss: 1.0257 - val_accuracy: 0.7248 - val_loss: 0.9549\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7036 - loss: 1.0137 - val_accuracy: 0.7305 - val_loss: 0.8891\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7021 - loss: 1.0200 - val_accuracy: 0.7387 - val_loss: 0.8968\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.6973 - loss: 1.0310 - val_accuracy: 0.7448 - val_loss: 0.8966\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7054 - loss: 1.0077 - val_accuracy: 0.7343 - val_loss: 0.9293\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7053 - loss: 1.0183 - val_accuracy: 0.7430 - val_loss: 0.8878\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6995 - loss: 1.0288 - val_accuracy: 0.7110 - val_loss: 0.9491\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.5905 - loss: 1.1144 - val_accuracy: 0.7857 - val_loss: 0.5844\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7699 - loss: 0.6385 - val_accuracy: 0.8152 - val_loss: 0.5295\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7967 - loss: 0.5635 - val_accuracy: 0.8171 - val_loss: 0.4923\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8160 - loss: 0.5124 - val_accuracy: 0.8076 - val_loss: 0.4922\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8251 - loss: 0.4890 - val_accuracy: 0.8200 - val_loss: 0.4931\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.8333 - loss: 0.4560 - val_accuracy: 0.8371 - val_loss: 0.4477\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8400 - loss: 0.4304 - val_accuracy: 0.8438 - val_loss: 0.4463\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8461 - loss: 0.4170 - val_accuracy: 0.8305 - val_loss: 0.4733\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8545 - loss: 0.3915 - val_accuracy: 0.8383 - val_loss: 0.4478\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8589 - loss: 0.3829 - val_accuracy: 0.8377 - val_loss: 0.4615\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5602 - loss: 2.5681 - val_accuracy: 0.7107 - val_loss: 1.0511\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6909 - loss: 1.1114 - val_accuracy: 0.7269 - val_loss: 0.9461\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6994 - loss: 1.0311 - val_accuracy: 0.7343 - val_loss: 0.9073\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7140 - loss: 0.9823 - val_accuracy: 0.7352 - val_loss: 0.8743\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7195 - loss: 0.9581 - val_accuracy: 0.7457 - val_loss: 0.8581\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7271 - loss: 0.9305 - val_accuracy: 0.7505 - val_loss: 0.8466\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7267 - loss: 0.9245 - val_accuracy: 0.7524 - val_loss: 0.8262\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7293 - loss: 0.9121 - val_accuracy: 0.7442 - val_loss: 0.8221\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7323 - loss: 0.9007 - val_accuracy: 0.7404 - val_loss: 0.8246\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7345 - loss: 0.8928 - val_accuracy: 0.7589 - val_loss: 0.8071\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6069 - loss: 1.0888 - val_accuracy: 0.7770 - val_loss: 0.6253\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7728 - loss: 0.6448 - val_accuracy: 0.7629 - val_loss: 0.7215\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7918 - loss: 0.5964 - val_accuracy: 0.8088 - val_loss: 0.5791\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7968 - loss: 0.6041 - val_accuracy: 0.7842 - val_loss: 0.6579\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7969 - loss: 0.5964 - val_accuracy: 0.8015 - val_loss: 0.6974\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7961 - loss: 0.6241 - val_accuracy: 0.8097 - val_loss: 0.5757\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7968 - loss: 0.6259 - val_accuracy: 0.7975 - val_loss: 0.6116\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7973 - loss: 0.6239 - val_accuracy: 0.7905 - val_loss: 0.6867\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7886 - loss: 0.6375 - val_accuracy: 0.7990 - val_loss: 0.6810\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7892 - loss: 0.6429 - val_accuracy: 0.8105 - val_loss: 0.7174\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5409 - loss: 2.4848 - val_accuracy: 0.5994 - val_loss: 1.2830\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6845 - loss: 1.0931 - val_accuracy: 0.7259 - val_loss: 0.9363\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7029 - loss: 1.0063 - val_accuracy: 0.7204 - val_loss: 0.9072\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7094 - loss: 0.9611 - val_accuracy: 0.7242 - val_loss: 0.8773\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7178 - loss: 0.9420 - val_accuracy: 0.7413 - val_loss: 0.8664\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7225 - loss: 0.9290 - val_accuracy: 0.7486 - val_loss: 0.8311\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7283 - loss: 0.9076 - val_accuracy: 0.7232 - val_loss: 0.8937\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7280 - loss: 0.8979 - val_accuracy: 0.7594 - val_loss: 0.7886\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7330 - loss: 0.8797 - val_accuracy: 0.7520 - val_loss: 0.8149\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7368 - loss: 0.8709 - val_accuracy: 0.7608 - val_loss: 0.7799\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4933 - loss: 1.3228 - val_accuracy: 0.7316 - val_loss: 0.6945\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6798 - loss: 0.8477 - val_accuracy: 0.7670 - val_loss: 0.6061\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7201 - loss: 0.7484 - val_accuracy: 0.7810 - val_loss: 0.5795\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7424 - loss: 0.7021 - val_accuracy: 0.7701 - val_loss: 0.6042\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7539 - loss: 0.6685 - val_accuracy: 0.7950 - val_loss: 0.5482\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7714 - loss: 0.6314 - val_accuracy: 0.8109 - val_loss: 0.5129\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7818 - loss: 0.6063 - val_accuracy: 0.8080 - val_loss: 0.5317\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7958 - loss: 0.5796 - val_accuracy: 0.8038 - val_loss: 0.5180\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8007 - loss: 0.5593 - val_accuracy: 0.8175 - val_loss: 0.4859\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7996 - loss: 0.5503 - val_accuracy: 0.8290 - val_loss: 0.5015\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.4808 - loss: 3.0685 - val_accuracy: 0.6815 - val_loss: 1.1016\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6490 - loss: 1.1781 - val_accuracy: 0.6754 - val_loss: 1.0284\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6620 - loss: 1.1327 - val_accuracy: 0.6922 - val_loss: 0.9974\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6742 - loss: 1.1114 - val_accuracy: 0.7090 - val_loss: 0.9942\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6711 - loss: 1.1044 - val_accuracy: 0.7183 - val_loss: 0.9552\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6782 - loss: 1.0976 - val_accuracy: 0.7107 - val_loss: 1.0112\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6769 - loss: 1.0987 - val_accuracy: 0.7076 - val_loss: 0.9488\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6778 - loss: 1.0898 - val_accuracy: 0.6451 - val_loss: 1.0790\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6821 - loss: 1.0833 - val_accuracy: 0.6821 - val_loss: 0.9980\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.6764 - loss: 1.1010 - val_accuracy: 0.7291 - val_loss: 0.9452\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5364 - loss: 1.2678 - val_accuracy: 0.7526 - val_loss: 0.6300\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.7553 - val_accuracy: 0.7802 - val_loss: 0.5690\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7584 - loss: 0.6628 - val_accuracy: 0.8091 - val_loss: 0.5253\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7773 - loss: 0.6200 - val_accuracy: 0.8067 - val_loss: 0.5147\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7935 - loss: 0.5732 - val_accuracy: 0.8190 - val_loss: 0.4953\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8007 - loss: 0.5497 - val_accuracy: 0.8272 - val_loss: 0.4743\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8058 - loss: 0.5320 - val_accuracy: 0.8259 - val_loss: 0.4911\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8199 - loss: 0.5100 - val_accuracy: 0.8297 - val_loss: 0.4615\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8244 - loss: 0.4904 - val_accuracy: 0.8280 - val_loss: 0.4649\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8275 - loss: 0.4738 - val_accuracy: 0.8257 - val_loss: 0.5052\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.5090 - loss: 2.3279 - val_accuracy: 0.6970 - val_loss: 1.1193\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6391 - loss: 1.2354 - val_accuracy: 0.7154 - val_loss: 0.9818\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6682 - loss: 1.1350 - val_accuracy: 0.6914 - val_loss: 0.9835\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6748 - loss: 1.0959 - val_accuracy: 0.7187 - val_loss: 0.9509\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6816 - loss: 1.0719 - val_accuracy: 0.7210 - val_loss: 0.9225\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6860 - loss: 1.0462 - val_accuracy: 0.7408 - val_loss: 0.8775\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6905 - loss: 1.0411 - val_accuracy: 0.7307 - val_loss: 0.8974\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7005 - loss: 1.0119 - val_accuracy: 0.7379 - val_loss: 0.8911\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7003 - loss: 1.0089 - val_accuracy: 0.7469 - val_loss: 0.8569\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7017 - loss: 0.9968 - val_accuracy: 0.7335 - val_loss: 0.8652\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5271 - loss: 1.2952 - val_accuracy: 0.7512 - val_loss: 0.6566\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7185 - loss: 0.7644 - val_accuracy: 0.7851 - val_loss: 0.5870\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7517 - loss: 0.6912 - val_accuracy: 0.8030 - val_loss: 0.5526\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7620 - loss: 0.6654 - val_accuracy: 0.8038 - val_loss: 0.5630\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7743 - loss: 0.6513 - val_accuracy: 0.8072 - val_loss: 0.5494\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7772 - loss: 0.6496 - val_accuracy: 0.7968 - val_loss: 0.6519\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7769 - loss: 0.6491 - val_accuracy: 0.8010 - val_loss: 0.6113\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7775 - loss: 0.6602 - val_accuracy: 0.8051 - val_loss: 0.6626\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7757 - loss: 0.6670 - val_accuracy: 0.7865 - val_loss: 0.6991\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7737 - loss: 0.6774 - val_accuracy: 0.7867 - val_loss: 0.7229\n",
      "Epoch 1/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.4881 - loss: 2.2768 - val_accuracy: 0.7093 - val_loss: 1.0808\n",
      "Epoch 2/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6547 - loss: 1.1966 - val_accuracy: 0.7150 - val_loss: 0.9753\n",
      "Epoch 3/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6737 - loss: 1.1077 - val_accuracy: 0.7164 - val_loss: 0.9455\n",
      "Epoch 4/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6880 - loss: 1.0646 - val_accuracy: 0.7276 - val_loss: 0.9136\n",
      "Epoch 5/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 1.0453 - val_accuracy: 0.7408 - val_loss: 0.8538\n",
      "Epoch 6/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7056 - loss: 1.0074 - val_accuracy: 0.7368 - val_loss: 0.8737\n",
      "Epoch 7/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7025 - loss: 0.9959 - val_accuracy: 0.6918 - val_loss: 0.9958\n",
      "Epoch 8/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7083 - loss: 0.9852 - val_accuracy: 0.7463 - val_loss: 0.8363\n",
      "Epoch 9/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7129 - loss: 0.9593 - val_accuracy: 0.7084 - val_loss: 0.8957\n",
      "Epoch 10/10\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7131 - loss: 0.9564 - val_accuracy: 0.7204 - val_loss: 0.8979\n"
     ]
    }
   ],
   "source": [
    "for layers in layer_configs:\n",
    "    for optimizer_class, optimizer_params in optimizers:  \n",
    "        for regularization in regularizations:\n",
    "            model = build_model(layers, dropout_rate=0.5, regularization=regularization)\n",
    "            optimizer = optimizer_class(**optimizer_params)\n",
    "            model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            \n",
    "            history_callback = History()\n",
    "\n",
    "            model.fit(X_train_norm, y_train, validation_data=(X_val_norm, y_val), #training\n",
    "                      epochs=10, batch_size=32, verbose=1, callbacks=[history_callback])\n",
    "            \n",
    "            history = history_callback.history\n",
    "            \n",
    "            for epoch in range(10):\n",
    "                epoch_results = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'layers': layers,\n",
    "                    'optimizer': optimizer.get_config()['name'],\n",
    "                    'regularization': 'None' if regularization is None else 'L2',\n",
    "                    'train_loss': history['loss'][epoch],\n",
    "                    'train_accuracy': history['accuracy'][epoch],\n",
    "                    'val_loss': history['val_loss'][epoch],\n",
    "                    'val_accuracy': history['val_accuracy'][epoch]\n",
    "                }\n",
    "                results.append(epoch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.850857138633728%\n",
      "Configurations: Layers: [512, 256, 128], Optimizer: adam, Regularization: None\n"
     ]
    }
   ],
   "source": [
    "best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(f\"Best Validation Accuracy: {best_result['val_accuracy']}%\")\n",
    "print(f\"Configurations: Layers: {best_result['layers']}, Optimizer: {best_result['optimizer']}, Regularization: {best_result['regularization']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best one so far is Layers: [512, 256, 128], Optimizer: adam, Regularization: None, Val Loss: 0.4152, Val Accuracy: 85%, wich is not bad but with pytorch we can try to get better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to TensorFlow's FNN, a fully connected network in PyTorch might not be optimal for clothing images due to its inability to capture spatial and local correlations efficiently in image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data reformatting and normalization for FNN\n",
    "X_train_fnn = X_train.reshape(-1, 32*32) / 255.0\n",
    "X_val_fnn = X_val.reshape(-1, 32*32) / 255.0\n",
    "X_test_fnn = X_test.reshape(-1, 32*32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_fnn = TensorDataset(torch.Tensor(X_train_fnn), torch.Tensor(y_train).long())\n",
    "val_tensor_fnn = TensorDataset(torch.Tensor(X_val_fnn), torch.Tensor(y_val).long())\n",
    "test_tensor_fnn = TensorDataset(torch.Tensor(X_test_fnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_fnn = DataLoader(train_tensor_fnn, batch_size=64, shuffle=True)\n",
    "val_loader_fnn = DataLoader(val_tensor_fnn, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(CustomFNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_size = input_size\n",
    "        for size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(current_size, size))\n",
    "            current_size = size\n",
    "        self.output_layer = nn.Linear(current_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # making sure that the data is solved correctly\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, layers, optimizer_name, regularization, epochs=10):\n",
    "    results = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader_fnn:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() #accumulate the training loss over all batches and count correct predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        val_loss, val_accuracy = validate(model, val_loader_fnn, criterion)\n",
    "\n",
    "        results.append({ #saving results\n",
    "            'epoch': epoch + 1,\n",
    "            'layers': layers,\n",
    "            'optimizer': optimizer_name,\n",
    "            'regularization': 'L2' if regularization else 'None',\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "      \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader_fnn, criterion):\n",
    "    model.eval()  # Putting the model into evaluation\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # disable gradients to speed up and reduce memory consumption\n",
    "        for inputs, labels in val_loader_fnn:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_loss /= len(val_loader_fnn)\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers\n",
    "layer_configs = [\n",
    "    [32, 64],\n",
    "    [64, 128],\n",
    "    [128, 256]\n",
    "]\n",
    "\n",
    "# optimizers\n",
    "optimizers = [\n",
    "    (optim.SGD, {'lr': 0.01}),\n",
    "    (optim.Adam, {'lr': 0.001}),\n",
    "    (optim.RMSprop, {'lr': 0.001})\n",
    "]\n",
    "\n",
    "# regularization\n",
    "regularizations = [\n",
    "    None,  \n",
    "    {'weight_decay': 0.01}  # L2 \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.8945, Train Accuracy: 33.42%, Val Loss: 1.3474, Val Accuracy: 53.96%\n",
      "Epoch 2/10, Train Loss: 1.1516, Train Accuracy: 58.08%, Val Loss: 1.0172, Val Accuracy: 63.12%\n",
      "Epoch 3/10, Train Loss: 0.9459, Train Accuracy: 64.97%, Val Loss: 0.8889, Val Accuracy: 67.68%\n",
      "Epoch 4/10, Train Loss: 0.8409, Train Accuracy: 68.95%, Val Loss: 0.8119, Val Accuracy: 70.08%\n",
      "Epoch 5/10, Train Loss: 0.7785, Train Accuracy: 71.45%, Val Loss: 0.7619, Val Accuracy: 72.78%\n",
      "Epoch 6/10, Train Loss: 0.7361, Train Accuracy: 73.03%, Val Loss: 0.7414, Val Accuracy: 72.25%\n",
      "Epoch 7/10, Train Loss: 0.7095, Train Accuracy: 73.87%, Val Loss: 0.8027, Val Accuracy: 69.09%\n",
      "Epoch 8/10, Train Loss: 0.6878, Train Accuracy: 74.65%, Val Loss: 0.7207, Val Accuracy: 72.10%\n",
      "Epoch 9/10, Train Loss: 0.6705, Train Accuracy: 75.45%, Val Loss: 0.6730, Val Accuracy: 76.04%\n",
      "Epoch 10/10, Train Loss: 0.6557, Train Accuracy: 75.98%, Val Loss: 0.6652, Val Accuracy: 76.51%\n",
      "Epoch 1/10, Train Loss: 1.9335, Train Accuracy: 32.57%, Val Loss: 1.3676, Val Accuracy: 51.50%\n",
      "Epoch 2/10, Train Loss: 1.1642, Train Accuracy: 55.67%, Val Loss: 1.0400, Val Accuracy: 60.67%\n",
      "Epoch 3/10, Train Loss: 0.9730, Train Accuracy: 63.64%, Val Loss: 0.9091, Val Accuracy: 66.93%\n",
      "Epoch 4/10, Train Loss: 0.8649, Train Accuracy: 68.48%, Val Loss: 0.8240, Val Accuracy: 71.10%\n",
      "Epoch 5/10, Train Loss: 0.7953, Train Accuracy: 71.22%, Val Loss: 0.7856, Val Accuracy: 72.19%\n",
      "Epoch 6/10, Train Loss: 0.7498, Train Accuracy: 72.92%, Val Loss: 0.7311, Val Accuracy: 74.13%\n",
      "Epoch 7/10, Train Loss: 0.7199, Train Accuracy: 73.80%, Val Loss: 0.7315, Val Accuracy: 72.17%\n",
      "Epoch 8/10, Train Loss: 0.6978, Train Accuracy: 74.46%, Val Loss: 0.7263, Val Accuracy: 73.81%\n",
      "Epoch 9/10, Train Loss: 0.6813, Train Accuracy: 75.05%, Val Loss: 0.7132, Val Accuracy: 74.10%\n",
      "Epoch 10/10, Train Loss: 0.6666, Train Accuracy: 75.64%, Val Loss: 0.6597, Val Accuracy: 76.17%\n",
      "Epoch 1/10, Train Loss: 0.8888, Train Accuracy: 67.34%, Val Loss: 0.6714, Val Accuracy: 75.90%\n",
      "Epoch 2/10, Train Loss: 0.6304, Train Accuracy: 77.22%, Val Loss: 0.6177, Val Accuracy: 77.83%\n",
      "Epoch 3/10, Train Loss: 0.5772, Train Accuracy: 79.02%, Val Loss: 0.5706, Val Accuracy: 79.47%\n",
      "Epoch 4/10, Train Loss: 0.5372, Train Accuracy: 80.56%, Val Loss: 0.5389, Val Accuracy: 80.59%\n",
      "Epoch 5/10, Train Loss: 0.5103, Train Accuracy: 81.41%, Val Loss: 0.5190, Val Accuracy: 81.39%\n",
      "Epoch 6/10, Train Loss: 0.4895, Train Accuracy: 82.15%, Val Loss: 0.5181, Val Accuracy: 81.54%\n",
      "Epoch 7/10, Train Loss: 0.4712, Train Accuracy: 82.74%, Val Loss: 0.5021, Val Accuracy: 81.79%\n",
      "Epoch 8/10, Train Loss: 0.4555, Train Accuracy: 83.14%, Val Loss: 0.5000, Val Accuracy: 81.79%\n",
      "Epoch 9/10, Train Loss: 0.4438, Train Accuracy: 83.62%, Val Loss: 0.4765, Val Accuracy: 82.17%\n",
      "Epoch 10/10, Train Loss: 0.4360, Train Accuracy: 84.05%, Val Loss: 0.4869, Val Accuracy: 82.32%\n",
      "Epoch 1/10, Train Loss: 0.8794, Train Accuracy: 68.06%, Val Loss: 0.6444, Val Accuracy: 76.74%\n",
      "Epoch 2/10, Train Loss: 0.6147, Train Accuracy: 77.50%, Val Loss: 0.6139, Val Accuracy: 77.92%\n",
      "Epoch 3/10, Train Loss: 0.5646, Train Accuracy: 79.51%, Val Loss: 0.5489, Val Accuracy: 80.19%\n",
      "Epoch 4/10, Train Loss: 0.5313, Train Accuracy: 80.73%, Val Loss: 0.5353, Val Accuracy: 80.67%\n",
      "Epoch 5/10, Train Loss: 0.5036, Train Accuracy: 81.70%, Val Loss: 0.5128, Val Accuracy: 81.33%\n",
      "Epoch 6/10, Train Loss: 0.4810, Train Accuracy: 82.37%, Val Loss: 0.4874, Val Accuracy: 82.36%\n",
      "Epoch 7/10, Train Loss: 0.4640, Train Accuracy: 82.85%, Val Loss: 0.4767, Val Accuracy: 82.69%\n",
      "Epoch 8/10, Train Loss: 0.4507, Train Accuracy: 83.29%, Val Loss: 0.4910, Val Accuracy: 81.90%\n",
      "Epoch 9/10, Train Loss: 0.4402, Train Accuracy: 83.82%, Val Loss: 0.4884, Val Accuracy: 82.59%\n",
      "Epoch 10/10, Train Loss: 0.4283, Train Accuracy: 84.07%, Val Loss: 0.4545, Val Accuracy: 83.47%\n",
      "Epoch 1/10, Train Loss: 0.8391, Train Accuracy: 68.92%, Val Loss: 0.6952, Val Accuracy: 74.51%\n",
      "Epoch 2/10, Train Loss: 0.6431, Train Accuracy: 76.33%, Val Loss: 0.7139, Val Accuracy: 74.57%\n",
      "Epoch 3/10, Train Loss: 0.5937, Train Accuracy: 78.25%, Val Loss: 0.7055, Val Accuracy: 74.04%\n",
      "Epoch 4/10, Train Loss: 0.5585, Train Accuracy: 79.59%, Val Loss: 0.5873, Val Accuracy: 78.70%\n",
      "Epoch 5/10, Train Loss: 0.5324, Train Accuracy: 80.43%, Val Loss: 0.5419, Val Accuracy: 80.48%\n",
      "Epoch 6/10, Train Loss: 0.5115, Train Accuracy: 81.26%, Val Loss: 0.5763, Val Accuracy: 78.15%\n",
      "Epoch 7/10, Train Loss: 0.4949, Train Accuracy: 81.95%, Val Loss: 0.5581, Val Accuracy: 80.76%\n",
      "Epoch 8/10, Train Loss: 0.4810, Train Accuracy: 82.31%, Val Loss: 0.5446, Val Accuracy: 80.50%\n",
      "Epoch 9/10, Train Loss: 0.4695, Train Accuracy: 82.86%, Val Loss: 0.5222, Val Accuracy: 81.26%\n",
      "Epoch 10/10, Train Loss: 0.4579, Train Accuracy: 83.15%, Val Loss: 0.4970, Val Accuracy: 82.72%\n",
      "Epoch 1/10, Train Loss: 0.8215, Train Accuracy: 69.31%, Val Loss: 0.8461, Val Accuracy: 69.18%\n",
      "Epoch 2/10, Train Loss: 0.6265, Train Accuracy: 76.92%, Val Loss: 0.6073, Val Accuracy: 78.38%\n",
      "Epoch 3/10, Train Loss: 0.5721, Train Accuracy: 78.89%, Val Loss: 0.5888, Val Accuracy: 79.37%\n",
      "Epoch 4/10, Train Loss: 0.5346, Train Accuracy: 80.55%, Val Loss: 0.6838, Val Accuracy: 73.09%\n",
      "Epoch 5/10, Train Loss: 0.5057, Train Accuracy: 81.50%, Val Loss: 0.5376, Val Accuracy: 81.30%\n",
      "Epoch 6/10, Train Loss: 0.4843, Train Accuracy: 82.22%, Val Loss: 0.5732, Val Accuracy: 79.83%\n",
      "Epoch 7/10, Train Loss: 0.4659, Train Accuracy: 82.87%, Val Loss: 0.5606, Val Accuracy: 79.12%\n",
      "Epoch 8/10, Train Loss: 0.4516, Train Accuracy: 83.24%, Val Loss: 0.4930, Val Accuracy: 82.19%\n",
      "Epoch 9/10, Train Loss: 0.4385, Train Accuracy: 83.97%, Val Loss: 0.5580, Val Accuracy: 79.58%\n",
      "Epoch 10/10, Train Loss: 0.4287, Train Accuracy: 84.09%, Val Loss: 0.4787, Val Accuracy: 82.46%\n",
      "Epoch 1/10, Train Loss: 1.8393, Train Accuracy: 39.57%, Val Loss: 1.3309, Val Accuracy: 55.73%\n",
      "Epoch 2/10, Train Loss: 1.1481, Train Accuracy: 59.43%, Val Loss: 1.0300, Val Accuracy: 62.34%\n",
      "Epoch 3/10, Train Loss: 0.9427, Train Accuracy: 65.99%, Val Loss: 0.8777, Val Accuracy: 68.25%\n",
      "Epoch 4/10, Train Loss: 0.8395, Train Accuracy: 69.50%, Val Loss: 0.8066, Val Accuracy: 71.70%\n",
      "Epoch 5/10, Train Loss: 0.7748, Train Accuracy: 71.69%, Val Loss: 0.7507, Val Accuracy: 73.05%\n",
      "Epoch 6/10, Train Loss: 0.7338, Train Accuracy: 73.06%, Val Loss: 0.7305, Val Accuracy: 73.54%\n",
      "Epoch 7/10, Train Loss: 0.7053, Train Accuracy: 73.90%, Val Loss: 0.6955, Val Accuracy: 74.74%\n",
      "Epoch 8/10, Train Loss: 0.6846, Train Accuracy: 74.82%, Val Loss: 0.6808, Val Accuracy: 75.16%\n",
      "Epoch 9/10, Train Loss: 0.6682, Train Accuracy: 75.48%, Val Loss: 0.6670, Val Accuracy: 75.71%\n",
      "Epoch 10/10, Train Loss: 0.6542, Train Accuracy: 75.88%, Val Loss: 0.6604, Val Accuracy: 76.38%\n",
      "Epoch 1/10, Train Loss: 1.8362, Train Accuracy: 36.87%, Val Loss: 1.3125, Val Accuracy: 57.60%\n",
      "Epoch 2/10, Train Loss: 1.1333, Train Accuracy: 61.08%, Val Loss: 0.9985, Val Accuracy: 65.43%\n",
      "Epoch 3/10, Train Loss: 0.9331, Train Accuracy: 67.01%, Val Loss: 0.8893, Val Accuracy: 67.12%\n",
      "Epoch 4/10, Train Loss: 0.8352, Train Accuracy: 69.97%, Val Loss: 0.7946, Val Accuracy: 72.00%\n",
      "Epoch 5/10, Train Loss: 0.7744, Train Accuracy: 72.05%, Val Loss: 0.7547, Val Accuracy: 72.84%\n",
      "Epoch 6/10, Train Loss: 0.7344, Train Accuracy: 73.45%, Val Loss: 0.7310, Val Accuracy: 73.96%\n",
      "Epoch 7/10, Train Loss: 0.7055, Train Accuracy: 74.34%, Val Loss: 0.8005, Val Accuracy: 70.91%\n",
      "Epoch 8/10, Train Loss: 0.6839, Train Accuracy: 74.99%, Val Loss: 0.6764, Val Accuracy: 75.96%\n",
      "Epoch 9/10, Train Loss: 0.6668, Train Accuracy: 75.73%, Val Loss: 0.6652, Val Accuracy: 75.89%\n",
      "Epoch 10/10, Train Loss: 0.6531, Train Accuracy: 76.21%, Val Loss: 0.6744, Val Accuracy: 76.30%\n",
      "Epoch 1/10, Train Loss: 0.8129, Train Accuracy: 70.11%, Val Loss: 0.6719, Val Accuracy: 75.12%\n",
      "Epoch 2/10, Train Loss: 0.5947, Train Accuracy: 78.30%, Val Loss: 0.5695, Val Accuracy: 79.45%\n",
      "Epoch 3/10, Train Loss: 0.5306, Train Accuracy: 80.66%, Val Loss: 0.5275, Val Accuracy: 80.74%\n",
      "Epoch 4/10, Train Loss: 0.4897, Train Accuracy: 81.94%, Val Loss: 0.5178, Val Accuracy: 80.86%\n",
      "Epoch 5/10, Train Loss: 0.4625, Train Accuracy: 82.91%, Val Loss: 0.4777, Val Accuracy: 82.59%\n",
      "Epoch 6/10, Train Loss: 0.4360, Train Accuracy: 83.86%, Val Loss: 0.4658, Val Accuracy: 82.70%\n",
      "Epoch 7/10, Train Loss: 0.4202, Train Accuracy: 84.24%, Val Loss: 0.4521, Val Accuracy: 83.87%\n",
      "Epoch 8/10, Train Loss: 0.4067, Train Accuracy: 85.01%, Val Loss: 0.4514, Val Accuracy: 83.77%\n",
      "Epoch 9/10, Train Loss: 0.3931, Train Accuracy: 85.38%, Val Loss: 0.4306, Val Accuracy: 84.27%\n",
      "Epoch 10/10, Train Loss: 0.3793, Train Accuracy: 85.73%, Val Loss: 0.4487, Val Accuracy: 83.45%\n",
      "Epoch 1/10, Train Loss: 0.8460, Train Accuracy: 68.71%, Val Loss: 0.6278, Val Accuracy: 77.37%\n",
      "Epoch 2/10, Train Loss: 0.5918, Train Accuracy: 78.32%, Val Loss: 0.5723, Val Accuracy: 79.50%\n",
      "Epoch 3/10, Train Loss: 0.5309, Train Accuracy: 80.80%, Val Loss: 0.5437, Val Accuracy: 80.21%\n",
      "Epoch 4/10, Train Loss: 0.4963, Train Accuracy: 81.61%, Val Loss: 0.5325, Val Accuracy: 79.83%\n",
      "Epoch 5/10, Train Loss: 0.4657, Train Accuracy: 82.89%, Val Loss: 0.4848, Val Accuracy: 82.63%\n",
      "Epoch 6/10, Train Loss: 0.4442, Train Accuracy: 83.68%, Val Loss: 0.4749, Val Accuracy: 83.07%\n",
      "Epoch 7/10, Train Loss: 0.4267, Train Accuracy: 84.17%, Val Loss: 0.4599, Val Accuracy: 83.26%\n",
      "Epoch 8/10, Train Loss: 0.4103, Train Accuracy: 84.64%, Val Loss: 0.4590, Val Accuracy: 83.24%\n",
      "Epoch 9/10, Train Loss: 0.3967, Train Accuracy: 85.23%, Val Loss: 0.4353, Val Accuracy: 84.36%\n",
      "Epoch 10/10, Train Loss: 0.3857, Train Accuracy: 85.67%, Val Loss: 0.4448, Val Accuracy: 83.47%\n",
      "Epoch 1/10, Train Loss: 0.7697, Train Accuracy: 70.67%, Val Loss: 0.6874, Val Accuracy: 74.19%\n",
      "Epoch 2/10, Train Loss: 0.5848, Train Accuracy: 78.00%, Val Loss: 0.6085, Val Accuracy: 77.62%\n",
      "Epoch 3/10, Train Loss: 0.5268, Train Accuracy: 80.43%, Val Loss: 0.5667, Val Accuracy: 79.71%\n",
      "Epoch 4/10, Train Loss: 0.4850, Train Accuracy: 82.05%, Val Loss: 0.5589, Val Accuracy: 79.37%\n",
      "Epoch 5/10, Train Loss: 0.4571, Train Accuracy: 83.14%, Val Loss: 0.5084, Val Accuracy: 82.08%\n",
      "Epoch 6/10, Train Loss: 0.4351, Train Accuracy: 83.76%, Val Loss: 0.4566, Val Accuracy: 83.47%\n",
      "Epoch 7/10, Train Loss: 0.4163, Train Accuracy: 84.42%, Val Loss: 0.6359, Val Accuracy: 77.71%\n",
      "Epoch 8/10, Train Loss: 0.3999, Train Accuracy: 85.17%, Val Loss: 0.4549, Val Accuracy: 83.39%\n",
      "Epoch 9/10, Train Loss: 0.3873, Train Accuracy: 85.50%, Val Loss: 0.4554, Val Accuracy: 83.10%\n",
      "Epoch 10/10, Train Loss: 0.3738, Train Accuracy: 86.04%, Val Loss: 0.5051, Val Accuracy: 80.80%\n",
      "Epoch 1/10, Train Loss: 0.7715, Train Accuracy: 70.81%, Val Loss: 0.6971, Val Accuracy: 72.76%\n",
      "Epoch 2/10, Train Loss: 0.5905, Train Accuracy: 78.38%, Val Loss: 0.7353, Val Accuracy: 74.23%\n",
      "Epoch 3/10, Train Loss: 0.5380, Train Accuracy: 80.31%, Val Loss: 0.5573, Val Accuracy: 80.06%\n",
      "Epoch 4/10, Train Loss: 0.5027, Train Accuracy: 81.53%, Val Loss: 0.5316, Val Accuracy: 81.22%\n",
      "Epoch 5/10, Train Loss: 0.4761, Train Accuracy: 82.51%, Val Loss: 0.5071, Val Accuracy: 81.66%\n",
      "Epoch 6/10, Train Loss: 0.4535, Train Accuracy: 83.30%, Val Loss: 0.5523, Val Accuracy: 79.94%\n",
      "Epoch 7/10, Train Loss: 0.4353, Train Accuracy: 83.86%, Val Loss: 0.5390, Val Accuracy: 80.84%\n",
      "Epoch 8/10, Train Loss: 0.4200, Train Accuracy: 84.48%, Val Loss: 0.5194, Val Accuracy: 80.55%\n",
      "Epoch 9/10, Train Loss: 0.4085, Train Accuracy: 84.82%, Val Loss: 0.4564, Val Accuracy: 83.60%\n",
      "Epoch 10/10, Train Loss: 0.3979, Train Accuracy: 85.25%, Val Loss: 0.5565, Val Accuracy: 78.48%\n",
      "Epoch 1/10, Train Loss: 1.8503, Train Accuracy: 38.62%, Val Loss: 1.3275, Val Accuracy: 50.91%\n",
      "Epoch 2/10, Train Loss: 1.1357, Train Accuracy: 60.12%, Val Loss: 1.0004, Val Accuracy: 64.44%\n",
      "Epoch 3/10, Train Loss: 0.9421, Train Accuracy: 66.15%, Val Loss: 0.8944, Val Accuracy: 69.24%\n",
      "Epoch 4/10, Train Loss: 0.8469, Train Accuracy: 69.68%, Val Loss: 0.8286, Val Accuracy: 70.30%\n",
      "Epoch 5/10, Train Loss: 0.7830, Train Accuracy: 71.88%, Val Loss: 0.7686, Val Accuracy: 73.39%\n",
      "Epoch 6/10, Train Loss: 0.7365, Train Accuracy: 73.48%, Val Loss: 0.7669, Val Accuracy: 73.12%\n",
      "Epoch 7/10, Train Loss: 0.7052, Train Accuracy: 74.45%, Val Loss: 0.6997, Val Accuracy: 74.61%\n",
      "Epoch 8/10, Train Loss: 0.6826, Train Accuracy: 75.19%, Val Loss: 0.7049, Val Accuracy: 73.94%\n",
      "Epoch 9/10, Train Loss: 0.6635, Train Accuracy: 75.81%, Val Loss: 0.6670, Val Accuracy: 76.38%\n",
      "Epoch 10/10, Train Loss: 0.6487, Train Accuracy: 76.55%, Val Loss: 0.6508, Val Accuracy: 76.67%\n",
      "Epoch 1/10, Train Loss: 1.8323, Train Accuracy: 42.67%, Val Loss: 1.2857, Val Accuracy: 57.22%\n",
      "Epoch 2/10, Train Loss: 1.1129, Train Accuracy: 61.09%, Val Loss: 0.9845, Val Accuracy: 64.76%\n",
      "Epoch 3/10, Train Loss: 0.9187, Train Accuracy: 66.79%, Val Loss: 0.8568, Val Accuracy: 69.62%\n",
      "Epoch 4/10, Train Loss: 0.8215, Train Accuracy: 70.10%, Val Loss: 0.8025, Val Accuracy: 71.66%\n",
      "Epoch 5/10, Train Loss: 0.7629, Train Accuracy: 72.23%, Val Loss: 0.7363, Val Accuracy: 73.50%\n",
      "Epoch 6/10, Train Loss: 0.7249, Train Accuracy: 73.72%, Val Loss: 0.7224, Val Accuracy: 73.68%\n",
      "Epoch 7/10, Train Loss: 0.6981, Train Accuracy: 74.48%, Val Loss: 0.7039, Val Accuracy: 75.12%\n",
      "Epoch 8/10, Train Loss: 0.6771, Train Accuracy: 75.24%, Val Loss: 0.6733, Val Accuracy: 76.02%\n",
      "Epoch 9/10, Train Loss: 0.6609, Train Accuracy: 75.65%, Val Loss: 0.7095, Val Accuracy: 74.04%\n",
      "Epoch 10/10, Train Loss: 0.6469, Train Accuracy: 76.28%, Val Loss: 0.6489, Val Accuracy: 76.82%\n",
      "Epoch 1/10, Train Loss: 0.7621, Train Accuracy: 72.07%, Val Loss: 0.6167, Val Accuracy: 77.50%\n",
      "Epoch 2/10, Train Loss: 0.5541, Train Accuracy: 79.73%, Val Loss: 0.5429, Val Accuracy: 80.23%\n",
      "Epoch 3/10, Train Loss: 0.4933, Train Accuracy: 81.72%, Val Loss: 0.4880, Val Accuracy: 82.65%\n",
      "Epoch 4/10, Train Loss: 0.4546, Train Accuracy: 83.00%, Val Loss: 0.4582, Val Accuracy: 83.16%\n",
      "Epoch 5/10, Train Loss: 0.4235, Train Accuracy: 84.14%, Val Loss: 0.4631, Val Accuracy: 82.99%\n",
      "Epoch 6/10, Train Loss: 0.4013, Train Accuracy: 84.98%, Val Loss: 0.4233, Val Accuracy: 84.76%\n",
      "Epoch 7/10, Train Loss: 0.3812, Train Accuracy: 85.60%, Val Loss: 0.4261, Val Accuracy: 84.59%\n",
      "Epoch 8/10, Train Loss: 0.3672, Train Accuracy: 86.05%, Val Loss: 0.4127, Val Accuracy: 84.97%\n",
      "Epoch 9/10, Train Loss: 0.3499, Train Accuracy: 86.65%, Val Loss: 0.4167, Val Accuracy: 84.97%\n",
      "Epoch 10/10, Train Loss: 0.3379, Train Accuracy: 87.21%, Val Loss: 0.4051, Val Accuracy: 84.95%\n",
      "Epoch 1/10, Train Loss: 0.7640, Train Accuracy: 71.67%, Val Loss: 0.5984, Val Accuracy: 78.44%\n",
      "Epoch 2/10, Train Loss: 0.5499, Train Accuracy: 80.01%, Val Loss: 0.5146, Val Accuracy: 81.77%\n",
      "Epoch 3/10, Train Loss: 0.4906, Train Accuracy: 82.00%, Val Loss: 0.4693, Val Accuracy: 83.10%\n",
      "Epoch 4/10, Train Loss: 0.4499, Train Accuracy: 83.49%, Val Loss: 0.4467, Val Accuracy: 84.00%\n",
      "Epoch 5/10, Train Loss: 0.4195, Train Accuracy: 84.51%, Val Loss: 0.4874, Val Accuracy: 81.41%\n",
      "Epoch 6/10, Train Loss: 0.3993, Train Accuracy: 85.08%, Val Loss: 0.4687, Val Accuracy: 83.26%\n",
      "Epoch 7/10, Train Loss: 0.3839, Train Accuracy: 85.61%, Val Loss: 0.4254, Val Accuracy: 84.69%\n",
      "Epoch 8/10, Train Loss: 0.3653, Train Accuracy: 86.36%, Val Loss: 0.4398, Val Accuracy: 83.60%\n",
      "Epoch 9/10, Train Loss: 0.3482, Train Accuracy: 86.91%, Val Loss: 0.4228, Val Accuracy: 84.13%\n",
      "Epoch 10/10, Train Loss: 0.3372, Train Accuracy: 87.30%, Val Loss: 0.4045, Val Accuracy: 85.45%\n",
      "Epoch 1/10, Train Loss: 0.7475, Train Accuracy: 71.46%, Val Loss: 0.6707, Val Accuracy: 75.77%\n",
      "Epoch 2/10, Train Loss: 0.5532, Train Accuracy: 79.48%, Val Loss: 0.7087, Val Accuracy: 74.02%\n",
      "Epoch 3/10, Train Loss: 0.4890, Train Accuracy: 81.72%, Val Loss: 0.5133, Val Accuracy: 81.09%\n",
      "Epoch 4/10, Train Loss: 0.4513, Train Accuracy: 83.10%, Val Loss: 0.5412, Val Accuracy: 79.77%\n",
      "Epoch 5/10, Train Loss: 0.4263, Train Accuracy: 84.01%, Val Loss: 0.4731, Val Accuracy: 82.65%\n",
      "Epoch 6/10, Train Loss: 0.4033, Train Accuracy: 84.77%, Val Loss: 0.4974, Val Accuracy: 80.72%\n",
      "Epoch 7/10, Train Loss: 0.3859, Train Accuracy: 85.46%, Val Loss: 0.5299, Val Accuracy: 80.44%\n",
      "Epoch 8/10, Train Loss: 0.3690, Train Accuracy: 86.05%, Val Loss: 0.4795, Val Accuracy: 82.36%\n",
      "Epoch 9/10, Train Loss: 0.3553, Train Accuracy: 86.57%, Val Loss: 0.4587, Val Accuracy: 82.97%\n",
      "Epoch 10/10, Train Loss: 0.3411, Train Accuracy: 87.05%, Val Loss: 0.4471, Val Accuracy: 83.89%\n",
      "Epoch 1/10, Train Loss: 0.7269, Train Accuracy: 72.34%, Val Loss: 0.6416, Val Accuracy: 74.69%\n",
      "Epoch 2/10, Train Loss: 0.5421, Train Accuracy: 79.96%, Val Loss: 0.6020, Val Accuracy: 78.30%\n",
      "Epoch 3/10, Train Loss: 0.4832, Train Accuracy: 82.12%, Val Loss: 0.5835, Val Accuracy: 77.22%\n",
      "Epoch 4/10, Train Loss: 0.4428, Train Accuracy: 83.37%, Val Loss: 0.5924, Val Accuracy: 78.25%\n",
      "Epoch 5/10, Train Loss: 0.4155, Train Accuracy: 84.49%, Val Loss: 0.4628, Val Accuracy: 82.97%\n",
      "Epoch 6/10, Train Loss: 0.3956, Train Accuracy: 85.15%, Val Loss: 0.5071, Val Accuracy: 80.50%\n",
      "Epoch 7/10, Train Loss: 0.3776, Train Accuracy: 85.59%, Val Loss: 0.5467, Val Accuracy: 80.08%\n",
      "Epoch 8/10, Train Loss: 0.3606, Train Accuracy: 86.38%, Val Loss: 0.4460, Val Accuracy: 84.00%\n",
      "Epoch 9/10, Train Loss: 0.3462, Train Accuracy: 86.82%, Val Loss: 0.5088, Val Accuracy: 81.35%\n",
      "Epoch 10/10, Train Loss: 0.3337, Train Accuracy: 87.46%, Val Loss: 0.4894, Val Accuracy: 81.85%\n"
     ]
    }
   ],
   "source": [
    "input_size = 1024  # input size\n",
    "output_size = 10\n",
    "for layers in layer_configs:\n",
    "    for optimizer_info, optimizer_params in optimizers:\n",
    "        for regularization in regularizations:\n",
    "            #Initializing the model \n",
    "            model = CustomFNN(input_size, layers, output_size)\n",
    "            optimizer = optimizer_info(model.parameters(), **optimizer_params)\n",
    "            optimizer_name = optimizer.__class__.__name__  # Получение имени класса оптимизатора\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # pass all necessary parameters to the teaching function\n",
    "            results = train_and_validate(\n",
    "                model, \n",
    "                train_loader_fnn, \n",
    "                val_loader_fnn, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                layers, \n",
    "                optimizer_name, \n",
    "                regularization, \n",
    "                epochs=10\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 84.0%\n",
      "Epoch: 8, Train Loss: 0.3605940985398387, Val Loss: 0.44598211294197176\n"
     ]
    }
   ],
   "source": [
    "best_result_pytorch = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(f\"Best Validation Accuracy: {best_result_pytorch['val_accuracy']}%\")\n",
    "print(f\"Epoch: {best_result_pytorch['epoch']}, Train Loss: {best_result_pytorch['train_loss']}, Val Loss: {best_result_pytorch['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A convolutional neural network in PyTorch is highly appropriate for processing clothing images, as it can leverage spatial relationships through its convolutional filters, making it better at recognizing patterns and features in image data, so probably its the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r = X_train.reshape(-1, 1, 32, 32) / 255.0\n",
    "X_val_r = X_val.reshape(-1, 1, 32, 32) / 255.0\n",
    "X_test_r = X_test.reshape(-1, 1, 32, 32) / 255.0 #normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = TensorDataset(torch.Tensor(X_train_r), torch.Tensor(y_train).long())\n",
    "val_tensor = TensorDataset(torch.Tensor(X_val_r), torch.Tensor(y_val).long())\n",
    "test_tensor = TensorDataset(torch.Tensor(X_test_r), torch.Tensor(y_test).long())\n",
    "\n",
    "#train test val tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_tensor, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, layers, regularization=None):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_channels = 1  # start with 1 chennel\n",
    "\n",
    "        for output_channels in layers:\n",
    "            self.layers.append(nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1))#adding to layers \n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.MaxPool2d(2))\n",
    "            input_channels = output_channels\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_channels * (32 // 2**len(layers))**2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        self.regularization = regularization\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers\n",
    "layer_configs = [\n",
    "    [32, 64],\n",
    "    [64, 128],\n",
    "    [128, 256]\n",
    "]\n",
    "\n",
    "# optimizers\n",
    "optimizers = [\n",
    "    (optim.SGD, {'lr': 0.01}),\n",
    "    (optim.Adam, {'lr': 0.001}),\n",
    "    (optim.RMSprop, {'lr': 0.001})\n",
    "]\n",
    "\n",
    "# regularization\n",
    "regularizations = [\n",
    "    None,  \n",
    "    {'weight_decay': 0.01}  # L2 \n",
    "]\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, layers, regularization, epochs=10):\n",
    "    results = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        val_loss, val_accuracy = validate(model, val_loader, criterion)\n",
    "\n",
    "        results.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'layers': layers,\n",
    "            'optimizer': optimizer.__class__.__name__,\n",
    "            'regularization': 'L2' if regularization else 'None',\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "      \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()  # evaluation\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.5118, Train Accuracy: 49.32%, Val Loss: 1.0702, Val Accuracy: 60.11%\n",
      "Epoch 2/10, Train Loss: 0.8278, Train Accuracy: 69.36%, Val Loss: 0.9480, Val Accuracy: 65.43%\n",
      "Epoch 3/10, Train Loss: 0.7253, Train Accuracy: 73.00%, Val Loss: 0.8070, Val Accuracy: 68.80%\n",
      "Epoch 4/10, Train Loss: 0.6608, Train Accuracy: 75.56%, Val Loss: 0.7169, Val Accuracy: 72.23%\n",
      "Epoch 5/10, Train Loss: 0.6159, Train Accuracy: 77.22%, Val Loss: 0.6503, Val Accuracy: 76.34%\n",
      "Epoch 6/10, Train Loss: 0.5822, Train Accuracy: 78.48%, Val Loss: 0.6667, Val Accuracy: 74.91%\n",
      "Epoch 7/10, Train Loss: 0.5500, Train Accuracy: 79.99%, Val Loss: 0.6064, Val Accuracy: 77.71%\n",
      "Epoch 8/10, Train Loss: 0.5310, Train Accuracy: 80.54%, Val Loss: 0.6530, Val Accuracy: 73.71%\n",
      "Epoch 9/10, Train Loss: 0.5087, Train Accuracy: 81.28%, Val Loss: 0.6221, Val Accuracy: 74.63%\n",
      "Epoch 10/10, Train Loss: 0.4936, Train Accuracy: 81.89%, Val Loss: 0.5098, Val Accuracy: 81.73%\n",
      "Epoch 1/10, Train Loss: 1.5736, Train Accuracy: 45.64%, Val Loss: 0.9145, Val Accuracy: 67.81%\n",
      "Epoch 2/10, Train Loss: 0.8208, Train Accuracy: 69.62%, Val Loss: 0.7693, Val Accuracy: 73.05%\n",
      "Epoch 3/10, Train Loss: 0.7196, Train Accuracy: 73.51%, Val Loss: 0.7353, Val Accuracy: 73.75%\n",
      "Epoch 4/10, Train Loss: 0.6561, Train Accuracy: 75.90%, Val Loss: 0.7597, Val Accuracy: 70.55%\n",
      "Epoch 5/10, Train Loss: 0.6147, Train Accuracy: 77.19%, Val Loss: 0.6307, Val Accuracy: 76.91%\n",
      "Epoch 6/10, Train Loss: 0.5793, Train Accuracy: 78.57%, Val Loss: 0.8194, Val Accuracy: 67.87%\n",
      "Epoch 7/10, Train Loss: 0.5518, Train Accuracy: 79.57%, Val Loss: 0.6024, Val Accuracy: 77.56%\n",
      "Epoch 8/10, Train Loss: 0.5316, Train Accuracy: 80.29%, Val Loss: 0.8168, Val Accuracy: 70.78%\n",
      "Epoch 9/10, Train Loss: 0.5131, Train Accuracy: 81.16%, Val Loss: 0.6958, Val Accuracy: 75.16%\n",
      "Epoch 10/10, Train Loss: 0.4952, Train Accuracy: 81.80%, Val Loss: 0.5142, Val Accuracy: 81.35%\n",
      "Epoch 1/10, Train Loss: 0.6461, Train Accuracy: 76.34%, Val Loss: 0.4529, Val Accuracy: 83.43%\n",
      "Epoch 2/10, Train Loss: 0.4142, Train Accuracy: 84.91%, Val Loss: 0.3897, Val Accuracy: 86.02%\n",
      "Epoch 3/10, Train Loss: 0.3568, Train Accuracy: 86.93%, Val Loss: 0.3633, Val Accuracy: 86.90%\n",
      "Epoch 4/10, Train Loss: 0.3191, Train Accuracy: 88.21%, Val Loss: 0.3398, Val Accuracy: 88.21%\n",
      "Epoch 5/10, Train Loss: 0.2934, Train Accuracy: 89.06%, Val Loss: 0.3236, Val Accuracy: 88.02%\n",
      "Epoch 6/10, Train Loss: 0.2664, Train Accuracy: 90.11%, Val Loss: 0.3161, Val Accuracy: 88.69%\n",
      "Epoch 7/10, Train Loss: 0.2430, Train Accuracy: 91.01%, Val Loss: 0.3140, Val Accuracy: 88.30%\n",
      "Epoch 8/10, Train Loss: 0.2215, Train Accuracy: 91.69%, Val Loss: 0.3060, Val Accuracy: 88.55%\n",
      "Epoch 9/10, Train Loss: 0.2040, Train Accuracy: 92.49%, Val Loss: 0.3174, Val Accuracy: 88.95%\n",
      "Epoch 10/10, Train Loss: 0.1828, Train Accuracy: 93.23%, Val Loss: 0.3207, Val Accuracy: 88.76%\n",
      "Epoch 1/10, Train Loss: 0.6403, Train Accuracy: 76.46%, Val Loss: 0.4807, Val Accuracy: 82.67%\n",
      "Epoch 2/10, Train Loss: 0.4262, Train Accuracy: 84.35%, Val Loss: 0.4001, Val Accuracy: 85.41%\n",
      "Epoch 3/10, Train Loss: 0.3699, Train Accuracy: 86.36%, Val Loss: 0.3706, Val Accuracy: 86.32%\n",
      "Epoch 4/10, Train Loss: 0.3317, Train Accuracy: 87.84%, Val Loss: 0.3431, Val Accuracy: 87.24%\n",
      "Epoch 5/10, Train Loss: 0.2968, Train Accuracy: 89.09%, Val Loss: 0.3180, Val Accuracy: 88.11%\n",
      "Epoch 6/10, Train Loss: 0.2748, Train Accuracy: 89.91%, Val Loss: 0.3052, Val Accuracy: 88.65%\n",
      "Epoch 7/10, Train Loss: 0.2508, Train Accuracy: 90.72%, Val Loss: 0.3181, Val Accuracy: 88.11%\n",
      "Epoch 8/10, Train Loss: 0.2300, Train Accuracy: 91.38%, Val Loss: 0.3341, Val Accuracy: 88.00%\n",
      "Epoch 9/10, Train Loss: 0.2082, Train Accuracy: 92.25%, Val Loss: 0.3530, Val Accuracy: 87.10%\n",
      "Epoch 10/10, Train Loss: 0.1900, Train Accuracy: 92.94%, Val Loss: 0.3285, Val Accuracy: 88.57%\n",
      "Epoch 1/10, Train Loss: 0.6172, Train Accuracy: 76.90%, Val Loss: 0.5151, Val Accuracy: 79.45%\n",
      "Epoch 2/10, Train Loss: 0.4104, Train Accuracy: 84.82%, Val Loss: 0.4594, Val Accuracy: 83.03%\n",
      "Epoch 3/10, Train Loss: 0.3551, Train Accuracy: 86.77%, Val Loss: 0.3936, Val Accuracy: 85.22%\n",
      "Epoch 4/10, Train Loss: 0.3175, Train Accuracy: 88.23%, Val Loss: 0.3556, Val Accuracy: 86.61%\n",
      "Epoch 5/10, Train Loss: 0.2898, Train Accuracy: 89.08%, Val Loss: 0.3511, Val Accuracy: 86.99%\n",
      "Epoch 6/10, Train Loss: 0.2660, Train Accuracy: 90.00%, Val Loss: 0.3092, Val Accuracy: 88.86%\n",
      "Epoch 7/10, Train Loss: 0.2449, Train Accuracy: 90.91%, Val Loss: 0.3798, Val Accuracy: 86.70%\n",
      "Epoch 8/10, Train Loss: 0.2271, Train Accuracy: 91.52%, Val Loss: 0.3406, Val Accuracy: 87.75%\n",
      "Epoch 9/10, Train Loss: 0.2083, Train Accuracy: 92.22%, Val Loss: 0.3365, Val Accuracy: 88.29%\n",
      "Epoch 10/10, Train Loss: 0.1928, Train Accuracy: 92.71%, Val Loss: 0.3123, Val Accuracy: 88.97%\n",
      "Epoch 1/10, Train Loss: 0.6182, Train Accuracy: 76.89%, Val Loss: 1.1940, Val Accuracy: 54.46%\n",
      "Epoch 2/10, Train Loss: 0.4076, Train Accuracy: 84.84%, Val Loss: 0.5238, Val Accuracy: 79.03%\n",
      "Epoch 3/10, Train Loss: 0.3529, Train Accuracy: 86.75%, Val Loss: 0.3519, Val Accuracy: 86.46%\n",
      "Epoch 4/10, Train Loss: 0.3150, Train Accuracy: 88.38%, Val Loss: 0.4133, Val Accuracy: 84.40%\n",
      "Epoch 5/10, Train Loss: 0.2883, Train Accuracy: 89.10%, Val Loss: 0.3586, Val Accuracy: 85.77%\n",
      "Epoch 6/10, Train Loss: 0.2632, Train Accuracy: 90.27%, Val Loss: 0.3553, Val Accuracy: 85.81%\n",
      "Epoch 7/10, Train Loss: 0.2428, Train Accuracy: 90.91%, Val Loss: 0.3377, Val Accuracy: 87.64%\n",
      "Epoch 8/10, Train Loss: 0.2229, Train Accuracy: 91.60%, Val Loss: 0.3376, Val Accuracy: 88.10%\n",
      "Epoch 9/10, Train Loss: 0.2059, Train Accuracy: 92.26%, Val Loss: 0.3322, Val Accuracy: 87.24%\n",
      "Epoch 10/10, Train Loss: 0.1880, Train Accuracy: 92.95%, Val Loss: 0.3730, Val Accuracy: 86.86%\n",
      "Epoch 1/10, Train Loss: 1.4943, Train Accuracy: 49.60%, Val Loss: 0.9169, Val Accuracy: 66.34%\n",
      "Epoch 2/10, Train Loss: 0.8144, Train Accuracy: 69.90%, Val Loss: 0.8698, Val Accuracy: 67.09%\n",
      "Epoch 3/10, Train Loss: 0.7075, Train Accuracy: 74.03%, Val Loss: 1.0118, Val Accuracy: 65.26%\n",
      "Epoch 4/10, Train Loss: 0.6423, Train Accuracy: 76.19%, Val Loss: 0.6841, Val Accuracy: 72.90%\n",
      "Epoch 5/10, Train Loss: 0.5957, Train Accuracy: 77.93%, Val Loss: 0.6262, Val Accuracy: 77.09%\n",
      "Epoch 6/10, Train Loss: 0.5596, Train Accuracy: 79.32%, Val Loss: 0.7438, Val Accuracy: 73.01%\n",
      "Epoch 7/10, Train Loss: 0.5322, Train Accuracy: 80.44%, Val Loss: 0.6481, Val Accuracy: 75.30%\n",
      "Epoch 8/10, Train Loss: 0.5099, Train Accuracy: 81.34%, Val Loss: 0.5677, Val Accuracy: 79.33%\n",
      "Epoch 9/10, Train Loss: 0.4939, Train Accuracy: 81.69%, Val Loss: 0.5463, Val Accuracy: 79.75%\n",
      "Epoch 10/10, Train Loss: 0.4772, Train Accuracy: 82.34%, Val Loss: 0.5352, Val Accuracy: 80.55%\n",
      "Epoch 1/10, Train Loss: 1.4095, Train Accuracy: 52.42%, Val Loss: 1.0102, Val Accuracy: 63.10%\n",
      "Epoch 2/10, Train Loss: 0.7964, Train Accuracy: 70.61%, Val Loss: 0.7423, Val Accuracy: 73.70%\n",
      "Epoch 3/10, Train Loss: 0.7009, Train Accuracy: 73.98%, Val Loss: 0.7050, Val Accuracy: 73.33%\n",
      "Epoch 4/10, Train Loss: 0.6419, Train Accuracy: 76.08%, Val Loss: 0.7312, Val Accuracy: 72.10%\n",
      "Epoch 5/10, Train Loss: 0.6005, Train Accuracy: 77.55%, Val Loss: 0.6709, Val Accuracy: 72.93%\n",
      "Epoch 6/10, Train Loss: 0.5680, Train Accuracy: 78.91%, Val Loss: 0.6147, Val Accuracy: 77.37%\n",
      "Epoch 7/10, Train Loss: 0.5429, Train Accuracy: 79.96%, Val Loss: 0.9118, Val Accuracy: 70.04%\n",
      "Epoch 8/10, Train Loss: 0.5218, Train Accuracy: 80.67%, Val Loss: 0.7243, Val Accuracy: 68.40%\n",
      "Epoch 9/10, Train Loss: 0.5046, Train Accuracy: 81.28%, Val Loss: 0.5415, Val Accuracy: 80.30%\n",
      "Epoch 10/10, Train Loss: 0.4872, Train Accuracy: 81.98%, Val Loss: 0.5179, Val Accuracy: 80.63%\n",
      "Epoch 1/10, Train Loss: 0.6154, Train Accuracy: 77.57%, Val Loss: 0.4524, Val Accuracy: 84.13%\n",
      "Epoch 2/10, Train Loss: 0.4014, Train Accuracy: 85.34%, Val Loss: 0.3812, Val Accuracy: 86.04%\n",
      "Epoch 3/10, Train Loss: 0.3431, Train Accuracy: 87.28%, Val Loss: 0.3844, Val Accuracy: 85.90%\n",
      "Epoch 4/10, Train Loss: 0.3014, Train Accuracy: 88.71%, Val Loss: 0.3331, Val Accuracy: 87.90%\n",
      "Epoch 5/10, Train Loss: 0.2702, Train Accuracy: 89.98%, Val Loss: 0.3034, Val Accuracy: 88.44%\n",
      "Epoch 6/10, Train Loss: 0.2431, Train Accuracy: 90.85%, Val Loss: 0.3128, Val Accuracy: 88.82%\n",
      "Epoch 7/10, Train Loss: 0.2183, Train Accuracy: 91.83%, Val Loss: 0.2933, Val Accuracy: 89.12%\n",
      "Epoch 8/10, Train Loss: 0.1952, Train Accuracy: 92.77%, Val Loss: 0.2913, Val Accuracy: 89.60%\n",
      "Epoch 9/10, Train Loss: 0.1766, Train Accuracy: 93.47%, Val Loss: 0.3239, Val Accuracy: 89.37%\n",
      "Epoch 10/10, Train Loss: 0.1588, Train Accuracy: 94.00%, Val Loss: 0.3175, Val Accuracy: 89.31%\n",
      "Epoch 1/10, Train Loss: 0.6246, Train Accuracy: 76.86%, Val Loss: 0.4763, Val Accuracy: 81.79%\n",
      "Epoch 2/10, Train Loss: 0.4084, Train Accuracy: 84.96%, Val Loss: 0.3906, Val Accuracy: 85.71%\n",
      "Epoch 3/10, Train Loss: 0.3491, Train Accuracy: 87.34%, Val Loss: 0.3513, Val Accuracy: 86.57%\n",
      "Epoch 4/10, Train Loss: 0.3058, Train Accuracy: 88.69%, Val Loss: 0.3230, Val Accuracy: 87.98%\n",
      "Epoch 5/10, Train Loss: 0.2755, Train Accuracy: 89.99%, Val Loss: 0.3059, Val Accuracy: 88.08%\n",
      "Epoch 6/10, Train Loss: 0.2474, Train Accuracy: 90.94%, Val Loss: 0.3387, Val Accuracy: 87.68%\n",
      "Epoch 7/10, Train Loss: 0.2219, Train Accuracy: 91.74%, Val Loss: 0.3095, Val Accuracy: 89.50%\n",
      "Epoch 8/10, Train Loss: 0.1992, Train Accuracy: 92.55%, Val Loss: 0.3190, Val Accuracy: 87.79%\n",
      "Epoch 9/10, Train Loss: 0.1783, Train Accuracy: 93.32%, Val Loss: 0.2980, Val Accuracy: 89.68%\n",
      "Epoch 10/10, Train Loss: 0.1599, Train Accuracy: 94.15%, Val Loss: 0.3139, Val Accuracy: 89.79%\n",
      "Epoch 1/10, Train Loss: 0.6635, Train Accuracy: 75.58%, Val Loss: 0.5791, Val Accuracy: 78.63%\n",
      "Epoch 2/10, Train Loss: 0.4345, Train Accuracy: 83.89%, Val Loss: 0.4537, Val Accuracy: 82.90%\n",
      "Epoch 3/10, Train Loss: 0.3778, Train Accuracy: 86.03%, Val Loss: 0.4676, Val Accuracy: 82.84%\n",
      "Epoch 4/10, Train Loss: 0.3387, Train Accuracy: 87.51%, Val Loss: 0.3825, Val Accuracy: 85.52%\n",
      "Epoch 5/10, Train Loss: 0.3098, Train Accuracy: 88.42%, Val Loss: 0.3951, Val Accuracy: 85.07%\n",
      "Epoch 6/10, Train Loss: 0.2861, Train Accuracy: 89.41%, Val Loss: 0.3381, Val Accuracy: 87.56%\n",
      "Epoch 7/10, Train Loss: 0.2669, Train Accuracy: 90.07%, Val Loss: 0.4076, Val Accuracy: 84.86%\n",
      "Epoch 8/10, Train Loss: 0.2488, Train Accuracy: 90.76%, Val Loss: 0.3375, Val Accuracy: 87.87%\n",
      "Epoch 9/10, Train Loss: 0.2340, Train Accuracy: 91.25%, Val Loss: 0.4191, Val Accuracy: 85.05%\n",
      "Epoch 10/10, Train Loss: 0.2212, Train Accuracy: 91.75%, Val Loss: 0.3228, Val Accuracy: 88.55%\n",
      "Epoch 1/10, Train Loss: 0.6644, Train Accuracy: 76.09%, Val Loss: 0.5097, Val Accuracy: 81.03%\n",
      "Epoch 2/10, Train Loss: 0.4215, Train Accuracy: 84.38%, Val Loss: 0.5473, Val Accuracy: 82.11%\n",
      "Epoch 3/10, Train Loss: 0.3659, Train Accuracy: 86.50%, Val Loss: 0.3686, Val Accuracy: 86.15%\n",
      "Epoch 4/10, Train Loss: 0.3246, Train Accuracy: 87.95%, Val Loss: 0.3667, Val Accuracy: 86.50%\n",
      "Epoch 5/10, Train Loss: 0.2950, Train Accuracy: 89.12%, Val Loss: 0.3249, Val Accuracy: 87.56%\n",
      "Epoch 6/10, Train Loss: 0.2689, Train Accuracy: 90.16%, Val Loss: 0.3122, Val Accuracy: 88.46%\n",
      "Epoch 7/10, Train Loss: 0.2469, Train Accuracy: 90.84%, Val Loss: 0.3475, Val Accuracy: 87.30%\n",
      "Epoch 8/10, Train Loss: 0.2287, Train Accuracy: 91.50%, Val Loss: 0.3045, Val Accuracy: 88.46%\n",
      "Epoch 9/10, Train Loss: 0.2114, Train Accuracy: 92.12%, Val Loss: 0.3699, Val Accuracy: 86.70%\n",
      "Epoch 10/10, Train Loss: 0.1957, Train Accuracy: 92.64%, Val Loss: 0.3122, Val Accuracy: 88.63%\n",
      "Epoch 1/10, Train Loss: 1.2213, Train Accuracy: 58.29%, Val Loss: 0.8171, Val Accuracy: 70.32%\n",
      "Epoch 2/10, Train Loss: 0.7550, Train Accuracy: 71.88%, Val Loss: 0.6984, Val Accuracy: 73.71%\n",
      "Epoch 3/10, Train Loss: 0.6643, Train Accuracy: 75.23%, Val Loss: 0.6939, Val Accuracy: 75.79%\n",
      "Epoch 4/10, Train Loss: 0.6052, Train Accuracy: 77.57%, Val Loss: 0.6325, Val Accuracy: 77.43%\n",
      "Epoch 5/10, Train Loss: 0.5650, Train Accuracy: 79.16%, Val Loss: 0.6373, Val Accuracy: 75.60%\n",
      "Epoch 6/10, Train Loss: 0.5334, Train Accuracy: 80.25%, Val Loss: 0.5947, Val Accuracy: 78.02%\n",
      "Epoch 7/10, Train Loss: 0.5088, Train Accuracy: 81.29%, Val Loss: 0.5590, Val Accuracy: 78.76%\n",
      "Epoch 8/10, Train Loss: 0.4885, Train Accuracy: 82.07%, Val Loss: 0.5001, Val Accuracy: 82.17%\n",
      "Epoch 9/10, Train Loss: 0.4715, Train Accuracy: 82.67%, Val Loss: 0.4930, Val Accuracy: 82.70%\n",
      "Epoch 10/10, Train Loss: 0.4557, Train Accuracy: 83.34%, Val Loss: 0.4945, Val Accuracy: 81.87%\n",
      "Epoch 1/10, Train Loss: 1.2809, Train Accuracy: 55.24%, Val Loss: 0.9843, Val Accuracy: 64.04%\n",
      "Epoch 2/10, Train Loss: 0.7752, Train Accuracy: 71.62%, Val Loss: 0.7806, Val Accuracy: 72.82%\n",
      "Epoch 3/10, Train Loss: 0.6717, Train Accuracy: 75.13%, Val Loss: 0.6288, Val Accuracy: 77.85%\n",
      "Epoch 4/10, Train Loss: 0.6044, Train Accuracy: 77.79%, Val Loss: 0.7006, Val Accuracy: 73.56%\n",
      "Epoch 5/10, Train Loss: 0.5645, Train Accuracy: 79.21%, Val Loss: 0.7854, Val Accuracy: 72.17%\n",
      "Epoch 6/10, Train Loss: 0.5313, Train Accuracy: 80.58%, Val Loss: 0.5208, Val Accuracy: 81.30%\n",
      "Epoch 7/10, Train Loss: 0.5110, Train Accuracy: 81.24%, Val Loss: 0.5744, Val Accuracy: 78.88%\n",
      "Epoch 8/10, Train Loss: 0.4915, Train Accuracy: 81.95%, Val Loss: 0.6433, Val Accuracy: 77.60%\n",
      "Epoch 9/10, Train Loss: 0.4730, Train Accuracy: 82.60%, Val Loss: 0.6597, Val Accuracy: 75.77%\n",
      "Epoch 10/10, Train Loss: 0.4594, Train Accuracy: 83.16%, Val Loss: 0.4717, Val Accuracy: 83.31%\n",
      "Epoch 1/10, Train Loss: 0.6069, Train Accuracy: 77.43%, Val Loss: 0.4380, Val Accuracy: 84.23%\n",
      "Epoch 2/10, Train Loss: 0.3948, Train Accuracy: 85.62%, Val Loss: 0.3708, Val Accuracy: 86.48%\n",
      "Epoch 3/10, Train Loss: 0.3343, Train Accuracy: 87.58%, Val Loss: 0.3394, Val Accuracy: 87.73%\n",
      "Epoch 4/10, Train Loss: 0.2952, Train Accuracy: 89.11%, Val Loss: 0.3111, Val Accuracy: 88.53%\n",
      "Epoch 5/10, Train Loss: 0.2624, Train Accuracy: 90.33%, Val Loss: 0.3126, Val Accuracy: 88.72%\n",
      "Epoch 6/10, Train Loss: 0.2362, Train Accuracy: 91.24%, Val Loss: 0.3072, Val Accuracy: 89.07%\n",
      "Epoch 7/10, Train Loss: 0.2102, Train Accuracy: 92.06%, Val Loss: 0.2935, Val Accuracy: 89.52%\n",
      "Epoch 8/10, Train Loss: 0.1886, Train Accuracy: 92.97%, Val Loss: 0.3049, Val Accuracy: 88.99%\n",
      "Epoch 9/10, Train Loss: 0.1651, Train Accuracy: 93.80%, Val Loss: 0.3101, Val Accuracy: 89.28%\n",
      "Epoch 10/10, Train Loss: 0.1438, Train Accuracy: 94.58%, Val Loss: 0.3332, Val Accuracy: 89.14%\n",
      "Epoch 1/10, Train Loss: 0.6045, Train Accuracy: 77.53%, Val Loss: 0.4554, Val Accuracy: 82.99%\n",
      "Epoch 2/10, Train Loss: 0.3879, Train Accuracy: 85.70%, Val Loss: 0.3590, Val Accuracy: 87.07%\n",
      "Epoch 3/10, Train Loss: 0.3293, Train Accuracy: 87.89%, Val Loss: 0.3529, Val Accuracy: 87.09%\n",
      "Epoch 4/10, Train Loss: 0.2875, Train Accuracy: 89.36%, Val Loss: 0.3273, Val Accuracy: 87.87%\n",
      "Epoch 5/10, Train Loss: 0.2595, Train Accuracy: 90.40%, Val Loss: 0.3066, Val Accuracy: 88.80%\n",
      "Epoch 6/10, Train Loss: 0.2322, Train Accuracy: 91.40%, Val Loss: 0.2966, Val Accuracy: 89.26%\n",
      "Epoch 7/10, Train Loss: 0.2075, Train Accuracy: 92.32%, Val Loss: 0.3069, Val Accuracy: 89.09%\n",
      "Epoch 8/10, Train Loss: 0.1858, Train Accuracy: 92.94%, Val Loss: 0.2833, Val Accuracy: 89.56%\n",
      "Epoch 9/10, Train Loss: 0.1638, Train Accuracy: 93.75%, Val Loss: 0.3538, Val Accuracy: 88.21%\n",
      "Epoch 10/10, Train Loss: 0.1417, Train Accuracy: 94.75%, Val Loss: 0.3107, Val Accuracy: 89.54%\n",
      "Epoch 1/10, Train Loss: 0.7335, Train Accuracy: 75.53%, Val Loss: 0.5274, Val Accuracy: 79.37%\n",
      "Epoch 2/10, Train Loss: 0.4232, Train Accuracy: 84.40%, Val Loss: 0.5533, Val Accuracy: 78.90%\n",
      "Epoch 3/10, Train Loss: 0.3586, Train Accuracy: 86.69%, Val Loss: 0.3991, Val Accuracy: 85.28%\n",
      "Epoch 4/10, Train Loss: 0.3207, Train Accuracy: 88.00%, Val Loss: 0.3345, Val Accuracy: 87.60%\n"
     ]
    }
   ],
   "source": [
    "for layers in layer_configs:\n",
    "    for optimizer_info, optimizer_params in optimizers:\n",
    "        for regularization in regularizations:\n",
    "            model = CustomCNN(layers, regularization)\n",
    "            optimizer = optimizer_info(model.parameters(), **optimizer_params)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            results = train_and_validate(model, train_loader, val_loader, optimizer, criterion, layers, regularization, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Configuration: Layers: [64, 128], Optimizer: Adam, Regularization: None, Val Accuracy: 89.71%\n"
     ]
    }
   ],
   "source": [
    "best_result_pytorch = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(f\"Best Validation Accuracy: {best_result_pytorch['val_accuracy']}%\")\n",
    "print(f\"Epoch: {best_result_pytorch['epoch']}, Train Loss: {best_result_pytorch['train_loss']}, Val Loss: {best_result_pytorch['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_channels = 1  \n",
    "\n",
    "        for output_channels in layers:\n",
    "            self.layers.append(nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.MaxPool2d(2))\n",
    "            input_channels = output_channels\n",
    "\n",
    "        # classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_channels * (32 // 2**len(layers))**2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN([64, 128])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam with best learning rate\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs} training complete.')\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = train_and_validate(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving trained model\n",
    "torch.save(final_model.state_dict(), 'final_best_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN([64, 128])\n",
    "\n",
    "#weights\n",
    "model.load_state_dict(torch.load('final_best_model.pth'))\n",
    "\n",
    "# evaluation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_tensor, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Перевод модели в режим оценки\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Использование CrossEntropyLoss для классификации\n",
    "\n",
    "    with torch.no_grad():  # Отключение градиентов для ускорения вычислений\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('evaluate.csv')\n",
    "X_test = test_data.drop('ID', axis=1).values\n",
    "X_test = X_test.reshape(-1, 1, 32, 32) / 255.0  \n",
    "test_ids = test_data['ID']\n",
    "\n",
    "# to tensor pytorch\n",
    "test_tensor = TensorDataset(torch.Tensor(X_test))\n",
    "\n",
    "# DataLoader\n",
    "test_loader = DataLoader(test_tensor, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN([64, 128])  \n",
    "model.load_state_dict(torch.load('final_best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# collecting predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs[0])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "#df to csv\n",
    "results_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
